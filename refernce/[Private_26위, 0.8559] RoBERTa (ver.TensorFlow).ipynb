{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112690,
     "status": "ok",
     "timestamp": 1645972270802,
     "user": {
      "displayName": "‍김태형[ 대학원석·박사통합과정재학 / 산업경영공학과 ]",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjFNpnkjN-Em0rOki5hhy0HR7yGAbxSpzCjHV0A=s64",
      "userId": "00288066936238655028"
     },
     "user_tz": -540
    },
    "id": "A1IbqGhzB7fy",
    "outputId": "fd656f36-afcb-4871-ee1c-557fd033af76"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# %matplotlib inline\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModel\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "import re\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import argparse\n",
    "# import wandb\n",
    "# from wandb.keras import WandbCallback\n",
    "# wandb.init(project=\"DACON_235978\", name=\"gnoeyheat\")\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"gnoeyheat\")\n",
    "parser.add_argument('--text_pretrained_model', default=\"roberta\", type=str)\n",
    "parser.add_argument('--text_len', default=300, type=int)\n",
    "parser.add_argument('--optimizer', default=\"sgd\", type=str) # sgd or adam\n",
    "parser.add_argument('--learning_rate', default=0.002, type=float)\n",
    "parser.add_argument('--loss', default='cc', type=str) # cc or fl\n",
    "parser.add_argument('--label_smoothing', default=0.1, type=float)\n",
    "parser.add_argument('--batch_size', default=32, type=int)\n",
    "parser.add_argument('--epochs', default=30, type=int)\n",
    "parser.add_argument('--validation_split', default=0.1, type=float)\n",
    "parser.add_argument('--seed', default=1011, type=int)\n",
    "args = parser.parse_args('')\n",
    "\n",
    "# wandb.config.update(args)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "text_pretrained_model = args.text_pretrained_model\n",
    "text_len = args.text_len\n",
    "BATCH_SIZE=args.batch_size\n",
    "EPOCHS=args.epochs\n",
    "VALIDATION_SPLIT=args.validation_split\n",
    "SEED=args.seed\n",
    "\n",
    "def set_seeds(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.text_pretrained_model == \"roberta\":\n",
    "    text_pretrained_model = \"klue/roberta-large\"\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(text_pretrained_model)\n",
    "# tokenizer.truncation_side = 'left'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "X_txt = train[\"overview\"]\n",
    "X_test_txt = test[\"overview\"]\n",
    "\n",
    "y = train[\"cat3\"]\n",
    "y_encoder = {key : value for key, value in zip(np.unique(y), range(len(np.unique(y))))}\n",
    "y = np.array([y_encoder[k] for k in y])\n",
    "\n",
    "X_txt.shape, X_test_txt.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(df):\n",
    "    df = df.apply(lambda x : re.sub('[^ ㄱ-ㅣ가-힣]+', ' ', x))\n",
    "    df = df.apply(lambda x : ' '.join(x.split()))\n",
    "    return df\n",
    "\n",
    "X_txt = text_cleaning(X_txt)\n",
    "X_test_txt = text_cleaning(X_test_txt)\n",
    "\n",
    "X_txt.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hanspell import spell_checker\n",
    "\n",
    "def text_correcting(df):\n",
    "    correct = []\n",
    "    for text in tqdm(df):\n",
    "        if len(text)<500:\n",
    "            correct.append(spell_checker.check(text).checked)\n",
    "        else:\n",
    "            temp = []\n",
    "            for i in range(0, len(text), 500):\n",
    "                temp.append(spell_checker.check(text[i:i+500]).checked)\n",
    "            correct.append(''.join(temp))\n",
    "    return pd.Series(correct, name=\"overview\")\n",
    "    \n",
    "X_txt = text_correcting(X_txt)\n",
    "X_test_txt = text_correcting(X_test_txt)\n",
    "\n",
    "train[\"overview\"] = X_txt\n",
    "test[\"overview\"] = X_test_txt\n",
    "train.to_csv(\"data/pp_train.csv\", index=False)\n",
    "test.to_csv(\"data/pp_test.csv\", index=False)\n",
    "\n",
    "X_txt.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"len\"] = train[\"overview\"].apply(tokenizer.tokenize).apply(len)\n",
    "test[\"len\"] = test[\"overview\"].apply(tokenizer.tokenize).apply(len)\n",
    "\n",
    "train[\"len\"].median(), test[\"len\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        sentence,\n",
    "        labels,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        include_targets=True,\n",
    "    ):\n",
    "        self.sentence = sentence\n",
    "        self.labels = labels\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.include_targets = include_targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.indexes = np.arange(len(self.sentence))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        sentence = self.sentence[indexes]\n",
    "\n",
    "        encoded = self.tokenizer.batch_encode_plus(\n",
    "            sentence.tolist(),\n",
    "            add_special_tokens=True,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=text_len,\n",
    "            return_tensors=\"tf\",\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
    "        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
    "        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
    "\n",
    "        if self.include_targets:\n",
    "            labels = np.array(self.labels[indexes], dtype=\"int32\")\n",
    "            return [input_ids, attention_masks, token_type_ids], labels\n",
    "        else:\n",
    "            return [input_ids, attention_masks, token_type_ids]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.RandomState(SEED).shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_txt, y, test_size=VALIDATION_SPLIT, random_state=SEED, stratify=y)\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_val = tf.keras.utils.to_categorical(y_val)\n",
    "\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = DataGenerator(\n",
    "    X_train.values, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_ds = DataGenerator(\n",
    "    X_val.values, y_val,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tf.keras.layers.Input(\n",
    "    shape=(text_len,), dtype=tf.int32, name=\"input_ids\"\n",
    ")\n",
    "attention_masks = tf.keras.layers.Input(\n",
    "    shape=(text_len,), dtype=tf.int32, name=\"attention_masks\"\n",
    ")\n",
    "token_type_ids = tf.keras.layers.Input(\n",
    "    shape=(text_len,), dtype=tf.int32, name=\"token_type_ids\"\n",
    ")\n",
    "\n",
    "bert_model = TFAutoModel.from_pretrained(text_pretrained_model, from_pt=True)\n",
    "bert_model.trainable = True\n",
    "\n",
    "bert_output = bert_model(\n",
    "    input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n",
    ")\n",
    "\n",
    "x = bert_output.last_hidden_state\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "output = layers.Dense(y_train.shape[1], activation=\"softmax\")(x)\n",
    "\n",
    "model = tf.keras.models.Model(\n",
    "    inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n",
    ")\n",
    "\n",
    "if args.optimizer == \"sgd\":\n",
    "    optim = tf.keras.optimizers.SGD(\n",
    "        learning_rate=lr, momentum=0.9\n",
    "    )\n",
    "if args.loss == \"cc\":\n",
    "    loss_function = tf.keras.losses.CategoricalCrossentropy(\n",
    "        label_smoothing=args.label_smoothing\n",
    "    )\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optim,\n",
    "    loss=loss_function,\n",
    "    metrics=tfa.metrics.F1Score(num_classes=y_train.shape[1], average=\"weighted\")\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = f\"load_model/{parser.description}\"\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 20:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "callback = [\n",
    "    tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_path,\n",
    "        monitor=\"val_f1_score\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        mode=\"max\",\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[callback],\n",
    "    validation_data=val_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['f1_score']\n",
    "val_acc = history.history['val_f1_score']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.plot(acc, label='Training Weighted-F1')\n",
    "plt.plot(val_acc, label='Validation Weighted-F1')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Weighted-F1')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_weighted_f1 = model.evaluate(val_ds)[1]\n",
    "print(f\"val_weighted_f1: {val_weighted_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = DataGenerator(\n",
    "    X_test_txt.values, None,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    include_targets=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob = []\n",
    "for i in range(test_ds.__len__()):\n",
    "    pred_prob.append(model.predict(test_ds.__getitem__(i)))\n",
    "pred_prob = np.vstack(pred_prob)\n",
    "pred = np.argmax(pred_prob, axis=1)\n",
    "\n",
    "y_decoder = {value : key for key, value in y_encoder.items()}\n",
    "result = np.array([y_decoder[v] for v in pred])\n",
    "\n",
    "pd.Series(result).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"data/sample_submission.csv\")\n",
    "submission[\"cat3\"] = result\n",
    "submission.to_csv(f\"{parser.description}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPxWDbXdBZDPv2XiwW8C1k0",
   "collapsed_sections": [],
   "mount_file_id": "13o4BpF8zzuXcEiNVlVG2KwTXMW1Y55_v",
   "name": "test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
